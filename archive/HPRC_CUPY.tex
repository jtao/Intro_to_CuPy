\documentclass[aspectratio=169,11pt]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{graphicx}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]
\usepackage[utf8]{inputenc}
\usepackage{inconsolata}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue}

\setminted[python]{frame=lines, fontsize=\footnotesize}

\title{Practical GPU Computing with CuPy}
\subtitle{From NumPy Compatibility to Custom Kernels and Production Deployment}
\author[J. Tao]{Jian Tao}
\institute[Texas A\&M University]{College of Performance Visualization \& Fine Arts,\\Texas A\&M Institute of Data Science, \\Texas A\&M High Performance Research Computing, \\Texas A\&M University\\College Station, TX}
\date{\today}

% TOC at start of each section
\AtBeginSection[]{
  \begin{frame}{Module Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

% ─────────────────────────────────────────────────────────────
% Slide 1: Title
% ─────────────────────────────────────────────────────────────
\begin{frame}
  \titlepage
  \note{Welcome! Goals: accelerate array workloads via GPU with CuPy; bridge NumPy ecosystem; master memory management, streams, custom kernels, interop with PyTorch/JAX, and production deployment best practices.}
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 2: Agenda
% ─────────────────────────────────────────────────────────────
\begin{frame}{Course Agenda}
  \tableofcontents
  \note{12 modules, 80 slides total. Mix of theory, hands-on code, profiling patterns, debugging, and advanced topics including DLPack interop and container optimization.}
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{CuPy Overview and Philosophy}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 3
% ─────────────────────────────────────────────────────────────
\begin{frame}{What is CuPy?}
CuPy is a NumPy/SciPy-compatible GPU array library accelerating computations via CUDA and ROCm backends, offering drop-in replacement APIs for many array operations. It provides ndarray, sparse matrices, and routines with the same API surface as NumPy and SciPy. 

\vspace{0.3cm}
\textbf{Key goals:}
\begin{itemize}
  \item Complete NumPy/SciPy API coverage for drop-in migration 
  \item Leverage CUDA libraries (cuBLAS, cuFFT, cuSOLVER) for performance 
  \item Enable advanced CUDA features without deep GPU expertise 
\end{itemize}
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 4
% ─────────────────────────────────────────────────────────────
\begin{frame}{Core Design Principle}
\textbf{Drop-in paradigm:} Replace \texttt{numpy} with \texttt{cupy} and \texttt{scipy} with \texttt{cupyx.scipy} to shift compute to GPU with minimal code change when APIs are compatible. 

\vspace{0.3cm}
This enables rapid prototyping and gradual migration of existing codebases, maintaining familiar syntax while exploiting GPU parallelism for large-scale data processing. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 5
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Hello, CuPy – First Program}
\begin{minted}{python}
import cupy as cp

# Create array on GPU
x = cp.arange(6).reshape(2, 3).astype('f')
# [[0. 1. 2.]
#  [3. 4. 5.]]

# Reduction along axis 1
y = x.sum(axis=1)
print(y)   # array([ 3., 12.], dtype=float32)
\end{minted}

\vspace{0.2cm}
This mirrors NumPy's syntax but transparently allocates GPU memory and dispatches CUDA kernels for operations. 
\end{frame}
% ═════════════════════════════════════════════════════════════
\section{Arrays, Memory Layout, and Data Transfer}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 9
% ─────────────────────────────────────────────────────────────
\begin{frame}{Creating Arrays on GPU}
\texttt{cupy.array} mirrors NumPy's constructor, supporting dtype, order (C/Fortran), and copy flags. Arrays allocate device memory and support asynchronous host-to-device (H2D) transfers when used with streams. 

\vspace{0.3cm}
\textbf{Options:}
\begin{itemize}
  \item \texttt{cp.array(host\_data)} – copy from host NumPy array 
  \item \texttt{cp.zeros, cp.ones, cp.empty} – initialize directly on GPU 
  \item \texttt{cp.asarray} – zero-copy view if already CuPy array 
\end{itemize}
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 10
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{cupy.array Basics and Options}
\begin{minted}{python}
import numpy as np
import cupy as cp

# Host NumPy array
a = np.arange(8, dtype=np.int32)

# H2D copy to GPU (blocking by default)
x = cp.array(a, copy=True)

# Direct GPU allocation
y = cp.zeros((2, 4), dtype=cp.float32, order='C')
z = cp.ones_like(y, dtype=cp.float64)

# Inspect strides
print(y.strides)  # (16, 4) for row-major float32
\end{minted}

\vspace{0.2cm}
H2D copies are synchronous unless wrapped in a stream context; manage stream ordering to prevent data races. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 11
% ─────────────────────────────────────────────────────────────
\begin{frame}{Dtypes, Byte Order, and Strides}
CuPy supports all NumPy dtypes (int8–64, float16/32/64, complex64/128, structured). When copying big-endian NumPy arrays, bytes are swapped to little-endian for GPU compatibility. 

\vspace{0.3cm}
\textbf{Memory order:}
\begin{itemize}
  \item \texttt{order='C'} – row-major (default) 
  \item \texttt{order='F'} – column-major (matches Fortran/MATLAB) 
\end{itemize}

Stride-aware indexing and broadcasting work identically to NumPy; ensure layout matches downstream kernel expectations for peak memory bandwidth. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 12
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Pinned Memory for Faster Transfers}
\begin{minted}{python}
import cupy as cp
import numpy as np

# Allocate pinned host buffer (non-pageable)
pinned_pool = cp.cuda.PinnedMemoryPool()
cp.cuda.set_pinned_memory_allocator(pinned_pool.malloc)

with cp.cuda.pinned_memory():
    h = np.random.rand(1000000).astype(np.float32)

# H2D transfer benefits from higher PCIe bandwidth
x = cp.asarray(h)

# Check pool stats
print(pinned_pool.n_free_blocks())
\end{minted}

\vspace{0.2cm}
Pinned (page-locked) memory improves H2D/D2H throughput and enables overlap with compute via streams. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{NumPy Compatibility and Migration}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 13
% ─────────────────────────────────────────────────────────────
\begin{frame}{Drop-in Replacement Paradigm}
Most NumPy operations work identically: broadcasting, fancy indexing, reductions (\texttt{sum, mean, std}), and linear algebra (\texttt{dot, @, linalg.*}) map to GPU when using CuPy arrays. 

\vspace{0.3cm}
\textbf{Automatic dispatch:} CuPy leverages cuBLAS for GEMM, cuFFT for FFT, and cuSOLVER for decompositions, providing performance on par with native CUDA code for standard operations. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 14
% ─────────────────────────────────────────────────────────────
\begin{frame}{Key Behavioral Differences}
\textbf{Intentional divergences from NumPy:} 
\begin{itemize}
  \item Out-of-bounds integer indexing may wrap instead of raising \texttt{IndexError} 
  \item Certain dtype promotion rules differ subtly 
  \item Unsupported: some advanced NumPy features (e.g., \texttt{np.vectorize}, certain string ops) 
\end{itemize}

\vspace{0.3cm}
Always test ported code for correctness; consult the differences guide for edge cases before migration. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 15
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Migrating Existing NumPy Code}
\begin{minted}{python}
# Before: NumPy on CPU
import numpy as np
x = np.random.rand(1000, 1000)
y = (x - x.mean(axis=0)) / x.std(axis=0)
z = np.linalg.svd(y, full_matrices=False)

# After: CuPy on GPU (identical syntax)
import cupy as cp
x = cp.random.rand(1000, 1000)
y = (x - x.mean(axis=0)) / x.std(axis=0)
u, s, vt = cp.linalg.svd(y, full_matrices=False)
# Result stays on GPU; use .get() only when needed
\end{minted}

\vspace{0.2cm}
Avoid round-tripping arrays to host in hot loops; keep pipeline GPU-resident for maximum speedup. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Universal Functions (ufuncs) and Element-wise Ops}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 16
% ─────────────────────────────────────────────────────────────
\begin{frame}{Ufunc Support in CuPy}
CuPy implements NumPy's universal functions (ufuncs) with full type signature support, \texttt{out} parameters, and methods like \texttt{accumulate}, \texttt{reduce}, \texttt{outer}. 

\vspace{0.3cm}
\textbf{Ufunc methods:} 
\begin{itemize}
  \item \texttt{cp.add.reduce(x)} – sum all elements 
  \item \texttt{cp.add.accumulate(x)} – prefix sum (scan) 
  \item \texttt{cp.multiply.outer(x, y)} – Cartesian product 
\end{itemize}

Each method dispatches optimized GPU kernels, exploiting parallelism for large arrays. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 17
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Using Built-in Ufuncs}
\begin{minted}{python}
import cupy as cp

x = cp.linspace(-3, 3, 7, dtype=cp.float32)

# Elementwise unary ufunc
y = cp.tanh(x)

# Reduction via ufunc method
total = cp.add.reduce(x)

# Prefix sum (inclusive scan)
cumulative = cp.add.accumulate(x)
print(cumulative)
# array([-3., -5., -6., -6., -5., -3.,  0.], dtype=float32)
\end{minted}

\vspace{0.2cm}
Ufuncs execute on GPU and support broadcasting; combine with \texttt{out=} to reuse buffers and reduce allocations. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 18
% ─────────────────────────────────────────────────────────────
\begin{frame}{cupy.ufunc Class and Metadata}
The \texttt{cupy.ufunc} object exposes attributes: 
\begin{itemize}
  \item \texttt{name} – kernel function name 
  \item \texttt{nin, nout} – number of input/output arrays 
  \item \texttt{types} – list of supported dtype signatures 
\end{itemize}

\vspace{0.3cm}
Use introspection to verify supported dtypes before dispatching custom ufuncs or when debugging type mismatches. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 19
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Custom Elementwise Ufunc with ElementwiseKernel}
\begin{minted}{python}
import cupy as cp

# Define custom ufunc: y = x^2
square = cp.ElementwiseKernel(
    'float32 x',        # input
    'float32 y',        # output
    'y = x * x;',       # operation (C syntax)
    'square_f32'        # kernel name
)

data = cp.arange(8, dtype=cp.float32)
result = square(data)
print(result)
# array([  0.,   1.,   4.,   9.,  16.,  25.,  36.,  49.], dtype=float32)
\end{minted}

\vspace{0.2cm}
\texttt{ElementwiseKernel} JIT-compiles CUDA code at first call; subsequent invocations reuse cached kernel. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Random Number Generation and Statistics}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 20
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{GPU Random Number Generation}
\begin{minted}{python}
import cupy as cp

# Modern API (NumPy 1.17+ style)
rng = cp.random.default_rng(seed=123)
x = rng.normal(loc=0, scale=1, size=(1_000_000,))

# Compute statistics on GPU
mean = x.mean()
std = x.std()

# Legacy API still supported
y = cp.random.rand(1000, 1000)
\end{minted}

\vspace{0.2cm}
Random generation uses cuRAND; all operations stay on device. Avoid \texttt{.get()} until final results needed on host. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 21
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Reductions and Scans at Scale}
\begin{minted}{python}
x = cp.arange(10_000_000, dtype=cp.float32)

# Global reductions (single-pass where possible)
s = x.sum()
mn = x.min()
mx = x.max()

# Prefix sum (inclusive scan via Thrust)
prefix = cp.cumsum(x)

# Argmax/argmin for index of extrema
idx = cp.argmax(x)
\end{minted}

\vspace{0.2cm}
Large reductions exploit GPU parallelism; CuPy dispatches optimized kernels from Thrust/CUB libraries for peak throughput. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Linear Algebra with cuBLAS and cuSOLVER}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 22
% ─────────────────────────────────────────────────────────────
\begin{frame}{Linear Algebra Dispatch}
CuPy's \texttt{linalg} module leverages: 
\begin{itemize}
  \item \textbf{cuBLAS} – matrix multiplication (GEMM), vector operations (BLAS Level 1/2/3) 
  \item \textbf{cuSOLVER} – LU, QR, SVD, Cholesky decompositions 
  \item \textbf{Tensor cores} – automatic use of low-precision acceleration (FP16/TF32) on modern GPUs 
\end{itemize}

\vspace{0.3cm}
These libraries deliver near-peak FLOPS for standard dense algebra workloads. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 23
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Matrix Operations and Decompositions}
\begin{minted}{python}
A = cp.random.rand(2048, 2048, dtype=cp.float32)
B = cp.random.rand(2048, 2048, dtype=cp.float32)

# Matrix multiplication via @ operator (calls cuBLAS GEMM)
C = A @ B

# Singular value decomposition (thin SVD)
u, s, vt = cp.linalg.svd(A, full_matrices=False)

# Eigendecomposition, matrix inversion, etc.
eigvals = cp.linalg.eigvalsh(A @ A.T)  # symmetric
\end{minted}

\vspace{0.2cm}
Use \texttt{float32} or mixed precision where acceptable to maximize throughput on tensor cores. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 24
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Batched and Vectorized Operations}
\begin{minted}{python}
# Batch of small matrices (100 matrices, 64x64 each)
X = cp.random.rand(100, 64, 64, dtype=cp.float32)

# Batched inversion (loops internally if no batch API)
X_inv = cp.linalg.inv(X)

# For large batch counts, consider manual loop with streams
from cupy.cuda import Stream
s = Stream()
with s:
    for i in range(100):
        X_inv[i] = cp.linalg.inv(X[i])
s.synchronize()
\end{minted}

\vspace{0.2cm}
Batch operations may loop under the hood; use streams to overlap compute when beneficial. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{FFT and SciPy API Coverage}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 25
% ─────────────────────────────────────────────────────────────
\begin{frame}{FFT Acceleration via cuFFT}
CuPy's \texttt{cp.fft} module mirrors \texttt{numpy.fft} and dispatches to cuFFT for 1D/2D/nD transforms: 
\begin{itemize}
  \item \texttt{cp.fft.fft, rfft, ifft} – complex and real FFTs 
  \item \texttt{cp.fft.fft2, fftn} – multidimensional transforms 
  \item \texttt{cp.fft.fftshift, ifftshift} – frequency-domain reordering 
\end{itemize}

\vspace{0.3cm}
Large FFTs exploit GPU memory bandwidth; plan caching reduces overhead on repeated calls. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 26
% ─────────────────────────────────────────────────────────────
\begin{frame}{cupyx.scipy – SciPy on GPU}
\texttt{cupyx.scipy} brings a subset of SciPy functionality to GPU: 
\begin{itemize}
  \item \texttt{cupyx.scipy.signal} – convolution, filtering, spectral analysis 
  \item \texttt{cupyx.scipy.sparse} – sparse matrix formats (CSR, COO, CSC) 
  \item \texttt{cupyx.scipy.linalg} – extended linear algebra beyond NumPy 
  \item \texttt{cupyx.scipy.fft} – FFT wrappers (overlaps with \texttt{cp.fft}) 
\end{itemize}

\vspace{0.3cm}
Coverage is partial; check documentation for supported functions before porting SciPy-heavy codebases. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 27
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{GPU 2D Convolution Example}
\begin{minted}{python}
import cupy as cp
from cupyx.scipy import signal

# Large image (4K×4K)
img = cp.random.rand(4096, 4096).astype(cp.float32)

# Gaussian-like kernel (31×31 Hanning window)
kernel = cp.outer(cp.hanning(31), cp.hanning(31))
kernel /= kernel.sum()  # normalize

# 2D convolution on GPU
out = signal.convolve2d(img, kernel, mode='same', boundary='wrap')

print(out.shape, out.dtype)
# (4096, 4096) float32
\end{minted}

\vspace{0.2cm}
Convolution mirrors SciPy's API but runs on GPU, leveraging parallel threads for spatial operations. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Custom Kernels: ElementwiseKernel, RawKernel, RawModule}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 28
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{ElementwiseKernel for Fused Operations}
\begin{minted}{python}
scale_bias = cp.ElementwiseKernel(
    'float32 x, float32 s, float32 b',  # inputs
    'float32 y',                         # output
    'y = s * x + b;',                    # C++ operation
    'scale_bias'                         # name
)

data = cp.arange(8, dtype=cp.float32)
result = scale_bias(data, 0.5, 1.0)
# result = 0.5 * data + 1.0
print(result)
# array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5], dtype=float32)
\end{minted}

\vspace{0.2cm}
Fuses scalar arithmetic into single kernel launch; reduces memory traffic vs chaining separate ufuncs. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 29
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{RawKernel for Full CUDA Control}
\begin{minted}{python}
code = r'''
extern "C" __global__
void saxpy(const float* x, const float* y, float* z,
           float a, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) z[i] = a * x[i] + y[i];
}'''
saxpy = cp.RawKernel(code, 'saxpy')
n = 1 << 20  # 1M elements
x = cp.random.rand(n, dtype=cp.float32)
y = cp.random.rand(n, dtype=cp.float32)
z = cp.empty(n, dtype=cp.float32)
# Launch with grid/block dimensions
blk = 256
grd = (n + blk - 1) // blk
saxpy((grd,), (blk,), (x, y, z, cp.float32(2.0), n))
\end{minted}

\vspace{0.1cm}
\texttt{RawKernel} JIT-compiles CUDA C++; cached per device for reuse. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 30
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{RawKernel with Shared Memory Reduction}
\begin{minted}{python}
code = r'''
extern "C" __global__
void block_sum(const float* x, float* out, int n) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + tid;
    sdata[tid] = (i < n) ? x[i] : 0.0f;
    __syncthreads();
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) sdata[tid] += sdata[tid + s];
        __syncthreads();
    }
    if (tid == 0) out[blockIdx.x] = sdata[0];
}'''
kernel = cp.RawKernel(code, 'block_sum')
# Launch with shared memory size (3rd arg to __call__)
\end{minted}

\vspace{0.1cm}
Shared memory tiling reduces global memory accesses; critical for bandwidth-bound reductions. 
\end{frame}


% ─────────────────────────────────────────────────────────────
% Slide 31 (NEW)
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{RawModule for Multiple Kernels}
\begin{minted}{python}
code = r'''
extern "C" __global__ void kernel_a(float* x, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) x[i] *= 2.0f;
}
extern "C" __global__ void kernel_b(float* x, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) x[i] += 1.0f;
}'''

mod = cp.RawModule(code=code)
ka = mod.get_function('kernel_a')
kb = mod.get_function('kernel_b')

x = cp.ones(1024, dtype=cp.float32)
ka((4,), (256,), (x, 1024))  # x *= 2
kb((4,), (256,), (x, 1024))  # x += 1
print(x[:5])  # [3. 3. 3. 3. 3.]
\end{minted}

\vspace{0.1cm}
\texttt{RawModule} compiles once; retrieve multiple kernels from the same source. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Memory Management: Pools, Streams, Events}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 32
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Memory Pools for Efficient Allocation}
\begin{minted}{python}
mp = cp.get_default_memory_pool()
pp = cp.get_default_pinned_memory_pool()

x = cp.empty((1024, 1024), dtype=cp.float32)

# Inspect pool usage
print(f"Used: {mp.used_bytes() / 1e6:.2f} MB")
print(f"Total allocated: {mp.total_bytes() / 1e6:.2f} MB")

# Free unused blocks back to OS
mp.free_all_blocks()
\end{minted}

\vspace{0.2cm}
Memory pools cache allocations to amortize cudaMalloc/cudaFree overhead; monitor usage with \texttt{nvidia-smi} to avoid OOM. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 33
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Streams for Concurrent Kernel Execution}
\begin{minted}{python}
s1 = cp.cuda.Stream(non_blocking=True)
s2 = cp.cuda.Stream(non_blocking=True)

with s1:
    x = cp.random.rand(1_000_000, dtype=cp.float32)
    y = cp.sin(x)

with s2:
    a = cp.random.rand(1_000_000, dtype=cp.float32)
    b = cp.cos(a)

s1.synchronize()
s2.synchronize()
\end{minted}

\vspace{0.2cm}
Non-blocking streams enable concurrent kernel launches and overlapping H2D/D2H transfers when hardware permits. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 34
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Events for Device-side Timing}
\begin{minted}{python}
start = cp.cuda.Event()
end = cp.cuda.Event()

start.record()
# Kernel or computation here
z = x @ y.reshape(-1, 1)
end.record()
end.synchronize()

elapsed_ms = cp.cuda.get_elapsed_time(start, end)
print(f"Kernel took {elapsed_ms:.3f} ms")
\end{minted}

\vspace{0.2cm}
CUDA events measure GPU execution time accurately, avoiding host-side scheduling noise. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 35
% ─────────────────────────────────────────────────────────────
\begin{frame}{Transfer Patterns and Data Residency}
\textbf{Best practices:} 
\begin{itemize}
  \item Keep data GPU-resident across entire pipeline 
  \item Batch H2D/D2H transfers; avoid per-iteration copies 
  \item Use pinned memory for large transfers 
  \item Overlap transfers with compute via streams 
\end{itemize}

\vspace{0.3cm}
Minimizing PCIe traffic is critical; profile with Nsight Systems to identify bottlenecks. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Performance Optimization and Profiling}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 42
% ─────────────────────────────────────────────────────────────
\begin{frame}{When GPUs Accelerate Workloads}
\textbf{Ideal use cases:} 
\begin{itemize}
  \item Large arrays ($>10^6$ elements) 
  \item Compute-intensive kernels (high arithmetic intensity) 
  \item Parallelizable operations (minimal data dependencies) 
\end{itemize}

\vspace{0.3cm}
\textbf{Avoid:}
\begin{itemize}
  \item Small arrays with frequent H2D/D2H copies 
  \item Tight Python loops calling single-element ops 
  \item Algorithms with heavy branching or irregular memory access 
\end{itemize}
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 43
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{End-to-End Timing with Synchronization}
\begin{minted}{python}
import time
import cupy as cp

# Ensure all prior kernels complete
cp.cuda.Stream.null.synchronize()

t0 = time.perf_counter()

# Compute workload
x = cp.random.rand(4096, 4096, dtype=cp.float32)
y = cp.linalg.svd(x, full_matrices=False)

# Wait for GPU to finish
cp.cuda.Stream.null.synchronize()

elapsed = time.perf_counter() - t0
print(f"Elapsed: {elapsed:.3f} s")
\end{minted}

\vspace{0.1cm}
Always synchronize before/after timing; kernel launches are asynchronous by default. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 44
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{cupyx.profiler.benchmark Utility}
\begin{minted}{python}
from cupyx.profiler import benchmark

def my_func(a, b):
    return 3 * cp.sin(-a) * b

a = 0.5 - cp.random.rand(10000)
b = cp.random.rand(10000)

# Run n_repeat times; report CPU/GPU time
result = benchmark(my_func, (a, b), n_repeat=1000,
                   n_warmup=10)
print(result)
# GPU: mean=0.12ms, std=0.01ms
# CPU: mean=0.15ms, std=0.02ms
\end{minted}

\vspace{0.1cm}
\texttt{benchmark} automates timing with warm-up, synchronization, and statistics collection. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 45
% ─────────────────────────────────────────────────────────────
\begin{frame}{Kernel Fusion and Launch Overhead}
\textbf{Problem:} Small kernels suffer from launch latency ($\sim$5--10 $\mu s$ per launch). 

\vspace{0.3cm}
\textbf{Solutions:} 
\begin{itemize}
  \item Fuse multiple elementwise ops into single \texttt{ElementwiseKernel} 
  \item Batch inputs to process multiple arrays per launch 
  \item Use vectorized CuPy expressions that auto-fuse where supported 
\end{itemize}

\vspace{0.3cm}
Fusion reduces memory traffic and kernel count; critical for bandwidth-bound workloads. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 46
% ─────────────────────────────────────────────────────────────
\begin{frame}{Bandwidth vs Compute Bound Analysis}
\textbf{Bandwidth-bound:} Memory transfer time dominates compute. 
\begin{itemize}
  \item Optimize: reduce data movement, fuse kernels, increase arithmetic intensity 
\end{itemize}

\vspace{0.3cm}
\textbf{Compute-bound:} ALU utilization limits throughput. 
\begin{itemize}
  \item Optimize: use lower precision (FP16/TF32), exploit tensor cores 
\end{itemize}

\vspace{0.3cm}
Profile with Nsight Compute to measure achieved bandwidth and FLOPS vs theoretical peak. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 47
% ─────────────────────────────────────────────────────────────
\begin{frame}{Streams and Concurrency for Overlap}
Multiple streams enable: 
\begin{itemize}
  \item Concurrent kernel execution (independent data) 
  \item Overlapping H2D transfers with compute 
  \item Overlapping D2H copies with next iteration's compute 
\end{itemize}

\vspace{0.3cm}
Requires careful synchronization (events/barriers) to avoid data races; test with CUDA memcheck. 
\end{frame}

% ═════════════════════════════════════════════════════════════
\section{Case Study: 2D Convolution Pipeline}
% ═════════════════════════════════════════════════════════════

% ─────────────────────────────────────────────────────────────
% Slide 48
% ─────────────────────────────────────────────────────────────
\begin{frame}{Convolution Case Study Overview}
\textbf{Goal:} Accelerate 2D image convolution on 4K×4K images using GPU. 

\vspace{0.3cm}
\textbf{Steps:} 
\begin{enumerate}
  \item Allocate data directly on GPU 
  \item Apply convolution via \texttt{cupyx.scipy.signal} 
  \item Measure timing with CUDA events 
  \item Compare to CPU SciPy baseline for validation 
\end{enumerate}

This workflow demonstrates end-to-end GPU pipeline with minimal host interaction. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 49
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Preparing Data on GPU}
\begin{minted}{python}
import cupy as cp

# Create sparse input (Dirac comb) on GPU
diracs = cp.zeros((4096, 4096), dtype=cp.float32)
diracs[::512, ::512] = 1.0

# 2D Gaussian-like kernel (31×31 Hanning window)
gauss = cp.outer(cp.hanning(31), cp.hanning(31))
gauss /= gauss.sum()  # normalize to unit sum
\end{minted}

\vspace{0.2cm}
Allocate arrays directly on device; avoid copying from host to maintain GPU residency. 
\end{frame}

% ─────────────────────────────────────────────────────────────
% Slide 50
% ─────────────────────────────────────────────────────────────
\begin{frame}[fragile]{Running GPU Convolution with Timing}
\begin{minted}{python}
from cupyx.scipy import signal

t0 = cp.cuda.Event()
t1 = cp.cuda.Event()

t0.record()
blur = signal.convolve2d(diracs, gauss, mode='same',
                         boundary='wrap')
t1.record()
t1.synchronize()

elapsed_ms = cp.cuda.get_elapsed_time(t0, t1)
print(f"GPU convolution: {elapsed_ms:.2f} ms")
\end{minted}

\vspace{0.2cm}
CUDA events measure kernel execution time independently of Python GIL or host scheduling. 
\end{frame}

\end{document}